{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing (NLP) is the technology used to help machines to understand and learn text and language. With NLP data scientists aim to teach machines to understand what is said and written to make sense of the human language. It is used to apply machine learning algorithms to text and speech.\n",
    "\n",
    "- Tokenization. ...\n",
    "- Normalization. ...\n",
    "- Stemming. ...\n",
    "- Lemmatization. ...\n",
    "- Corpus. ...\n",
    "- Stop Words. ...\n",
    "- Parts-of-speech (POS) Tagging.\n",
    "- regular expression\n",
    "- bag of words\n",
    "- N grams\n",
    "- symantic analysis\n",
    "- syntactic analysis\n",
    "\n",
    "We can use NLP to create systems like speech recognition, machine translation, spam detection, text simplifications, question answering, autocomplete, predictive typing, sentiment analysis, document summarization and many more.\n",
    "\n",
    "1) edurka https://www.youtube.com/watch?v=05ONoGfmKvA&t=611s\n",
    "\n",
    "2) https://www.youtube.com/watch?v=WYge0KZBhe0&t=442s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Corpus\n",
    "\n",
    "In linguistics and NLP, corpus (literally Latin for body) refers to a collection of texts. Such collections may be formed of a single language of texts, or can span multiple languages -- there are numerous reasons for which multilingual corpora (the plural of corpus) may be useful. Corpora may also consist of themed texts (historical, Biblical, etc.). Corpora are generally solely used for statistical linguistic analysis and hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['words', 'wordnet.zip', 'wordnet', 'stopwords.zip', 'gutenberg', 'gutenberg.zip', 'words.zip', 'stopwords', 'movie_reviews.zip', 'movie_reviews']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find('corpora')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('gutenberg')#gentenberg is a corpus u need to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids() # cor[us is a collection of written texts, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
    "hamlet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can use this files for lot of analysis purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Barnardo and Francisco two Centinels . Barnardo . Who ' s there ? Fran . Nay answer me : Stand & vnfold your selfe Bar . Long liue the King Fran . Barnardo ? Bar . He Fran . You come most carefully vpon your houre Bar . ' Tis now strook twelue , get thee to bed Francisco Fran . For this releefe much thankes : ' Tis bitter cold , And I am sicke at heart Barn . Haue you had quiet Guard ? Fran . Not a Mouse stirring Barn . Well , goodnight . If you do meet Horatio and Marcellus , the Riuals of my Watch , bid them make hast . Enter Horatio and Marcellus . Fran . I thinke I heare them . Stand : who ' s there ? Hor . Friends to this ground Mar . And Leige - men to the Dane Fran . Giue you good night Mar . O farwel honest Soldier , who hath relieu ' d you ? Fra . Barnardo ha ' s my place : giue you goodnight . Exit Fran . Mar . Holla Barnardo Bar . Say , what is Horatio there ? Hor . A peece of him Bar . Welcome Horatio , welcome good Marcellus Mar . What , ha ' s this thing appear ' d againe to night Bar . I haue seene nothing Mar . Horatio saies , ' tis but our Fantasie , And will not let beleefe take hold of him Touching this dreaded sight , twice seene of vs , Therefore I haue intreated him along With vs , to watch the minutes of this Night , That if againe this Apparition come , He may approue our eyes , and speake to it Hor . Tush , tush , ' twill not appeare Bar . Sit downe a - while , And let vs once againe assaile your eares , That are so fortified against our Story , What we two Nights haue seene Hor . Well , sit we downe , And let vs heare Barnardo speake of this Barn . Last night of all , When yond same Starre that ' s Westward from the Pole Had made his course t ' illume that part of Heauen Where now it burnes , Marcellus and my selfe , The Bell then beating one Mar . Peace , breake thee of : Enter the Ghost . Looke where it comes againe Barn . In the same figure , like the King that ' s dead Mar . Thou art a Scholler ; speake to it Horatio Barn . Lookes it not like the King ? Marke it Horatio Hora . Most like : It harrowes me with fear & wonder Barn . It would be spoke too Mar . Question it Horatio Hor . What art "
     ]
    }
   ],
   "source": [
    "for word in hamlet[:500]:#first 500 words \n",
    "    print(word,sep=' ', end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is, generally, an early step in the NLP process, a step which splits longer strings of text into smaller pieces, or tokens. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI = '''Machine learning involves computers discovering how they can perform\n",
    "tasks without being explicitly programmed to do so. It involves computers learning \n",
    "from data provided so that they carry out certain tasks. For simple tasks assigned to computers, \n",
    "it is possible to program algorithms telling the machine how to execute all steps required \n",
    "to solve the problem at hand; on the computer's part, no learning is needed. For more advanced tasks, \n",
    "it can be challenging for a human to manually create the needed algorithms. In practice, \n",
    "it can turn out to be more effective to help the machine develop its own algorithm,\n",
    "rather than having human programmers specify every needed step'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (AI)# it string easy for tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine',\n",
       " 'learning',\n",
       " 'involves',\n",
       " 'computers',\n",
       " 'discovering',\n",
       " 'how',\n",
       " 'they',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'tasks',\n",
       " 'without',\n",
       " 'being',\n",
       " 'explicitly',\n",
       " 'programmed',\n",
       " 'to',\n",
       " 'do',\n",
       " 'so',\n",
       " '.',\n",
       " 'It',\n",
       " 'involves',\n",
       " 'computers',\n",
       " 'learning',\n",
       " 'from',\n",
       " 'data',\n",
       " 'provided',\n",
       " 'so',\n",
       " 'that',\n",
       " 'they',\n",
       " 'carry',\n",
       " 'out',\n",
       " 'certain',\n",
       " 'tasks',\n",
       " '.',\n",
       " 'For',\n",
       " 'simple',\n",
       " 'tasks',\n",
       " 'assigned',\n",
       " 'to',\n",
       " 'computers',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'program',\n",
       " 'algorithms',\n",
       " 'telling',\n",
       " 'the',\n",
       " 'machine',\n",
       " 'how',\n",
       " 'to',\n",
       " 'execute',\n",
       " 'all',\n",
       " 'steps',\n",
       " 'required',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'at',\n",
       " 'hand',\n",
       " ';',\n",
       " 'on',\n",
       " 'the',\n",
       " 'computer',\n",
       " \"'s\",\n",
       " 'part',\n",
       " ',',\n",
       " 'no',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'needed',\n",
       " '.',\n",
       " 'For',\n",
       " 'more',\n",
       " 'advanced',\n",
       " 'tasks',\n",
       " ',',\n",
       " 'it',\n",
       " 'can',\n",
       " 'be',\n",
       " 'challenging',\n",
       " 'for',\n",
       " 'a',\n",
       " 'human',\n",
       " 'to',\n",
       " 'manually',\n",
       " 'create',\n",
       " 'the',\n",
       " 'needed',\n",
       " 'algorithms',\n",
       " '.',\n",
       " 'In',\n",
       " 'practice',\n",
       " ',',\n",
       " 'it',\n",
       " 'can',\n",
       " 'turn',\n",
       " 'out',\n",
       " 'to',\n",
       " 'be',\n",
       " 'more',\n",
       " 'effective',\n",
       " 'to',\n",
       " 'help',\n",
       " 'the',\n",
       " 'machine',\n",
       " 'develop',\n",
       " 'its',\n",
       " 'own',\n",
       " 'algorithm',\n",
       " ',',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'having',\n",
       " 'human',\n",
       " 'programmers',\n",
       " 'specify',\n",
       " 'every',\n",
       " 'needed',\n",
       " 'step']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_tokenize = word_tokenize(AI)\n",
    "AI_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')#Punkt Sentence Tokenizer. This tokenizer divides a text into a list of sentences, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency distinct function\n",
    "#finding word counts of all words in paragraph\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10 = fdist.most_common(10)\n",
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many paragraphs we have\n",
    "from nltk.tokenize import blankline_tokenize\n",
    "AI_blank= blankline_tokenize(AI)\n",
    "len(AI_blank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  N-grams\n",
    "\n",
    "n-grams is another representation model for simplifying text selection contents. As opposed to the orderless representation of bag of words, n-grams modeling is interested in preserving contiguous sequences of N items from the text selection.\n",
    "\n",
    "An example of trigram (3-gram) model of the second sentence of the above example (\"There, there,\" said James. \"There, there.\") appears as a list representation below:\n",
    "\n",
    "   [\n",
    "   \n",
    "       \"there there said\",\n",
    "      \n",
    "      \"there said james\",\n",
    "      \n",
    "      \"said james there\",\n",
    "      \n",
    "      \"james there there\",\n",
    "   ]\n",
    "   \n",
    "   \n",
    "- N-grams: tokens of any number of consecuive written words known as Ngram\n",
    "- Bigrams: tokens of two consecuive written words known as BIgram\n",
    "- trigrams: tokens of three consecuive written words known as BIgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine',\n",
       " 'learning',\n",
       " 'involves',\n",
       " 'computers',\n",
       " 'discovering',\n",
       " 'how',\n",
       " 'they',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'tasks',\n",
       " 'without',\n",
       " 'being',\n",
       " 'explicitly',\n",
       " 'programmed']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'Machine learning involves computers discovering how they can perform tasks without being explicitly programmed'\n",
    "quotes_token = nltk.word_tokenize(string)#It actually returns the syllables from a single word. \n",
    "quotes_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'learning'),\n",
       " ('learning', 'involves'),\n",
       " ('involves', 'computers'),\n",
       " ('computers', 'discovering'),\n",
       " ('discovering', 'how'),\n",
       " ('how', 'they'),\n",
       " ('they', 'can'),\n",
       " ('can', 'perform'),\n",
       " ('perform', 'tasks'),\n",
       " ('tasks', 'without'),\n",
       " ('without', 'being'),\n",
       " ('being', 'explicitly'),\n",
       " ('explicitly', 'programmed')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_bigram = list(nltk.bigrams(quotes_token))\n",
    "quotes_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'learning', 'involves'),\n",
       " ('learning', 'involves', 'computers'),\n",
       " ('involves', 'computers', 'discovering'),\n",
       " ('computers', 'discovering', 'how'),\n",
       " ('discovering', 'how', 'they'),\n",
       " ('how', 'they', 'can'),\n",
       " ('they', 'can', 'perform'),\n",
       " ('can', 'perform', 'tasks'),\n",
       " ('perform', 'tasks', 'without'),\n",
       " ('tasks', 'without', 'being'),\n",
       " ('without', 'being', 'explicitly'),\n",
       " ('being', 'explicitly', 'programmed')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_trigram = list(nltk.trigrams(quotes_token))\n",
    "quotes_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine',),\n",
       " ('learning',),\n",
       " ('involves',),\n",
       " ('computers',),\n",
       " ('discovering',),\n",
       " ('how',),\n",
       " ('they',),\n",
       " ('can',),\n",
       " ('perform',),\n",
       " ('tasks',),\n",
       " ('without',),\n",
       " ('being',),\n",
       " ('explicitly',),\n",
       " ('programmed',)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_ngrams = list(nltk.ngrams(quotes_token,1))\n",
    "quotes_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'learning', 'involves', 'computers'),\n",
       " ('learning', 'involves', 'computers', 'discovering'),\n",
       " ('involves', 'computers', 'discovering', 'how'),\n",
       " ('computers', 'discovering', 'how', 'they'),\n",
       " ('discovering', 'how', 'they', 'can'),\n",
       " ('how', 'they', 'can', 'perform'),\n",
       " ('they', 'can', 'perform', 'tasks'),\n",
       " ('can', 'perform', 'tasks', 'without'),\n",
       " ('perform', 'tasks', 'without', 'being'),\n",
       " ('tasks', 'without', 'being', 'explicitly'),\n",
       " ('without', 'being', 'explicitly', 'programmed')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_ngrams = list(nltk.ngrams(quotes_token,4))# if u place 4 here it shows 4 gram\n",
    "quotes_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# diff b-w stemm and lemma\n",
    "- Stemming just removes or stems the last few characters of a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma. Sometimes, the same word can have multiple different Lemmas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming\n",
    "- Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes \n",
    "- normalise the words into root form / base form\n",
    "\n",
    "Stemming\n",
    "\n",
    "Stemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem.\n",
    "\n",
    "running → run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caresses : caress\n",
      "files : file\n",
      "dies : die\n",
      "mules : mule\n",
      "denied : deni\n",
      "died : die\n",
      "agreed : agre\n",
      "owened : owen\n",
      "humbled : humbl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "\n",
    "plurals = ['caresses','files','dies','mules','denied','died','agreed','owened','humbled']\n",
    "\n",
    "for word in plurals:\n",
    "    print(word, \":\", pst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "gave:gave\n",
      "given:given\n",
      "giving:give\n"
     ]
    }
   ],
   "source": [
    "word_to_stem = ['give','gave','given','giving']\n",
    "for words in word_to_stem:\n",
    "    print(words+\":\"+pst.stem(words))\n",
    "    #print(words,\":\",pst.stem(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lancaster stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : giv\n",
      "gave : gav\n",
      "given : giv\n",
      "giving : giv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer#Lancasterster is significantly more dynamic than the Porter Stemmer\n",
    "lst = LancasterStemmer()\n",
    "for words in word_to_stem:\n",
    "    print(words,\":\",lst.stem(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snow ball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "gave : gave\n",
      "given : given\n",
      "giving : give\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sst = SnowballStemmer('english')\n",
    "for words in word_to_stem:\n",
    "    print(words,\":\",sst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finish'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "stemmer = EnglishStemmer()\n",
    "stemmer.stem(\"finishes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "- emmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .\n",
    "\n",
    "Lemmatization\n",
    "\n",
    "Lemmatization is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word's lemma.\n",
    "\n",
    "For example, stemming the word \"better\" would fail to return its citation form (another word for lemma); however, lemmatization would result in the following:\n",
    "\n",
    "better → good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " corpus is a large body of natural language text used for accumulating statistics on natural language text. The plural is corpora. Corpora often include extra information such as a tag for each word indicating its part-of-speech, and perhaps the parse tree for each sentence. Hope that solves your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lem.lemmatize('corpora')#corpora stands for multiple corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "gave : gave\n",
      "given : given\n",
      "giving : giving\n"
     ]
    }
   ],
   "source": [
    "for words in word_to_stem:\n",
    "    print(words,':',word_lem.lemmatize(words))\n",
    "#the words is as it is , because we haven't assign POS tags\n",
    "#pos tag is tells us what exactly the given word is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "\n",
    "Stop words are those words which are filtered out before further processing of text, since these words contribute little to overall meaning, given that they are generally the most common words in a language. For instance, \"the,\" \"and,\" and \"a,\" while all required words in a particular passage, don't generally contribute greatly to one's understanding of content. As a simple example, the following panagram is just as legible if the stop words are removed:\n",
    "\n",
    "The quick brown fox jumps over the lazy dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')#you need to download it\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello he said and went\n"
     ]
    }
   ],
   "source": [
    "# define punctuation\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "my_str = \"Hello!!!, he said ---and went.\"\n",
    "\n",
    "# To take input from the user\n",
    "# my_str = input(\"Enter a string: \")\n",
    "\n",
    "# remove punctuation from the string\n",
    "no_punct = \"\"\n",
    "for char in my_str:\n",
    "    if char not in punctuations:\n",
    "        no_punct = no_punct + char\n",
    "\n",
    "# display the unpunctuated string\n",
    "print(no_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pos Partsofspeech \n",
    "\n",
    "A Part-Of-Speech Tagger (POS Tagger) is a piece of software that reads text in some language and assigns parts of speech to each word (and other token), such as noun, verb, adjective, etc.\n",
    "\n",
    "\n",
    "POS tagging consists of assigning a category tag to the tokenized parts of a sentence. The most popular POS tagging would be identifying words as nouns, verbs, adjectives, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'madan is a natural when it comes to swimming'\n",
    "sent_token= word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('madan', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('natural', 'JJ')]\n",
      "[('when', 'WRB')]\n",
      "[('it', 'PRP')]\n",
      "[('comes', 'VBZ')]\n",
      "[('to', 'TO')]\n",
      "[('swimming', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "#need to download\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "for token in sent_token:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jhon', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('eating', 'VBG')]\n",
      "[('a', 'DT')]\n",
      "[('delicious', 'JJ')]\n",
      "[('cake', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#pos tagging\n",
    "sent2 = 'Jhon is eating a delicious cake'\n",
    "sent2_token= word_tokenize(sent2)\n",
    "for token in sent2_token:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition(NER)\n",
    "\n",
    "Named-entity recognition is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chunking is picking a individual pieces of information and grouping them into bigger pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_sent = \"Indian Primeminister lives in NewDelhi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_tokens = word_tokenize(NE_sent)\n",
    "NE_tags = nltk.pos_tag(NE_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WIthout POS tagging it is hard to detect Named entity(NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Indian/JJ)\n",
      "  (ORGANIZATION Primeminister/NNP)\n",
      "  lives/VBZ\n",
      "  in/IN\n",
      "  (ORGANIZATION NewDelhi/NNP))\n"
     ]
    }
   ],
   "source": [
    "#need to download#nltk.download('maxent_ne_chunker')\n",
    "NE_NER = ne_chunk(NE_tags)\n",
    "print(NE_NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Syntax tree or a parse tree is a tree representation of different syntactic categories of a sentence. It helps us to understand the syntactical structure of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('cat', 'NN'),\n",
       " ('ate', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('mouse', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('was', 'VBD'),\n",
       " ('after', 'IN'),\n",
       " ('fresh', 'JJ'),\n",
       " ('cheese', 'NN')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = \" the big cat ate the little mouse who was after fresh cheese\"\n",
    "new_tokens = nltk.pos_tag(word_tokenize(new))\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer_np = r\"NP:{<DT>?<33>*<NN>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_parser = nltk.RegexpParser(grammer_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [('the', 'DT'), ('big', 'JJ'), Tree('NP', [('cat', 'NN')]), ('ate', 'VBD'), ('the', 'DT'), ('little', 'JJ'), Tree('NP', [('mouse', 'NN')]), ('who', 'WP'), ('was', 'VBD'), ('after', 'IN'), ('fresh', 'JJ'), Tree('NP', [('cheese', 'NN')])])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_result = chunk_parser.parse(new_tokens)\n",
    "chunk_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['words', 'wordnet.zip', 'wordnet', 'stopwords.zip', 'gutenberg', 'gutenberg.zip', 'words.zip', 'stopwords', 'movie_reviews.zip', 'movie_reviews']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find('corpora')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.categories())\n",
    "#nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      " \n",
      "['pos/cv000_29590.txt', 'pos/cv001_18431.txt', 'pos/cv002_15918.txt', 'pos/cv003_11664.txt', 'pos/cv004_11636.txt', 'pos/cv005_29443.txt', 'pos/cv006_15448.txt', 'pos/cv007_4968.txt', 'pos/cv008_29435.txt', 'pos/cv009_29592.txt', 'pos/cv010_29198.txt', 'pos/cv011_12166.txt', 'pos/cv012_29576.txt', 'pos/cv013_10159.txt', 'pos/cv014_13924.txt', 'pos/cv015_29439.txt', 'pos/cv016_4659.txt', 'pos/cv017_22464.txt', 'pos/cv018_20137.txt', 'pos/cv019_14482.txt', 'pos/cv020_8825.txt', 'pos/cv021_15838.txt', 'pos/cv022_12864.txt', 'pos/cv023_12672.txt', 'pos/cv024_6778.txt', 'pos/cv025_3108.txt', 'pos/cv026_29325.txt', 'pos/cv027_25219.txt', 'pos/cv028_26746.txt', 'pos/cv029_18643.txt', 'pos/cv030_21593.txt', 'pos/cv031_18452.txt', 'pos/cv032_22550.txt', 'pos/cv033_24444.txt', 'pos/cv034_29647.txt', 'pos/cv035_3954.txt', 'pos/cv036_16831.txt', 'pos/cv037_18510.txt', 'pos/cv038_9749.txt', 'pos/cv039_6170.txt', 'pos/cv040_8276.txt', 'pos/cv041_21113.txt', 'pos/cv042_10982.txt', 'pos/cv043_15013.txt', 'pos/cv044_16969.txt', 'pos/cv045_23923.txt', 'pos/cv046_10188.txt', 'pos/cv047_1754.txt', 'pos/cv048_16828.txt', 'pos/cv049_20471.txt', 'pos/cv050_11175.txt', 'pos/cv051_10306.txt', 'pos/cv052_29378.txt', 'pos/cv053_21822.txt', 'pos/cv054_4230.txt', 'pos/cv055_8338.txt', 'pos/cv056_13133.txt', 'pos/cv057_7453.txt', 'pos/cv058_8025.txt', 'pos/cv059_28885.txt', 'pos/cv060_10844.txt', 'pos/cv061_8837.txt', 'pos/cv062_23115.txt', 'pos/cv063_28997.txt', 'pos/cv064_24576.txt', 'pos/cv065_15248.txt', 'pos/cv066_10821.txt', 'pos/cv067_19774.txt', 'pos/cv068_13400.txt', 'pos/cv069_10801.txt', 'pos/cv070_12289.txt', 'pos/cv071_12095.txt', 'pos/cv072_6169.txt', 'pos/cv073_21785.txt', 'pos/cv074_6875.txt', 'pos/cv075_6500.txt', 'pos/cv076_24945.txt', 'pos/cv077_22138.txt', 'pos/cv078_14730.txt', 'pos/cv079_11933.txt', 'pos/cv080_13465.txt', 'pos/cv081_16582.txt', 'pos/cv082_11080.txt', 'pos/cv083_24234.txt', 'pos/cv084_13566.txt', 'pos/cv085_1381.txt', 'pos/cv086_18371.txt', 'pos/cv087_1989.txt', 'pos/cv088_24113.txt', 'pos/cv089_11418.txt', 'pos/cv090_0042.txt', 'pos/cv091_7400.txt', 'pos/cv092_28017.txt', 'pos/cv093_13951.txt', 'pos/cv094_27889.txt', 'pos/cv095_28892.txt', 'pos/cv096_11474.txt', 'pos/cv097_24970.txt', 'pos/cv098_15435.txt', 'pos/cv099_10534.txt', 'pos/cv100_11528.txt', 'pos/cv101_10175.txt', 'pos/cv102_7846.txt', 'pos/cv103_11021.txt', 'pos/cv104_18134.txt', 'pos/cv105_17990.txt', 'pos/cv106_16807.txt', 'pos/cv107_24319.txt', 'pos/cv108_15571.txt', 'pos/cv109_21172.txt', 'pos/cv110_27788.txt', 'pos/cv111_11473.txt', 'pos/cv112_11193.txt', 'pos/cv113_23102.txt', 'pos/cv114_18398.txt', 'pos/cv115_25396.txt', 'pos/cv116_28942.txt', 'pos/cv117_24295.txt', 'pos/cv118_28980.txt', 'pos/cv119_9867.txt', 'pos/cv120_4111.txt', 'pos/cv121_17302.txt', 'pos/cv122_7392.txt', 'pos/cv123_11182.txt', 'pos/cv124_4122.txt', 'pos/cv125_9391.txt', 'pos/cv126_28971.txt', 'pos/cv127_14711.txt', 'pos/cv128_29627.txt', 'pos/cv129_16741.txt', 'pos/cv130_17083.txt', 'pos/cv131_10713.txt', 'pos/cv132_5618.txt', 'pos/cv133_16336.txt', 'pos/cv134_22246.txt', 'pos/cv135_11603.txt', 'pos/cv136_11505.txt', 'pos/cv137_15422.txt', 'pos/cv138_12721.txt', 'pos/cv139_12873.txt', 'pos/cv140_7479.txt', 'pos/cv141_15686.txt', 'pos/cv142_22516.txt', 'pos/cv143_19666.txt', 'pos/cv144_5007.txt', 'pos/cv145_11472.txt', 'pos/cv146_18458.txt', 'pos/cv147_21193.txt', 'pos/cv148_16345.txt', 'pos/cv149_15670.txt', 'pos/cv150_12916.txt', 'pos/cv151_15771.txt', 'pos/cv152_8736.txt', 'pos/cv153_10779.txt', 'pos/cv154_9328.txt', 'pos/cv155_7308.txt', 'pos/cv156_10481.txt', 'pos/cv157_29372.txt', 'pos/cv158_10390.txt', 'pos/cv159_29505.txt', 'pos/cv160_10362.txt', 'pos/cv161_11425.txt', 'pos/cv162_10424.txt', 'pos/cv163_10052.txt', 'pos/cv164_22447.txt', 'pos/cv165_22619.txt', 'pos/cv166_11052.txt', 'pos/cv167_16376.txt', 'pos/cv168_7050.txt', 'pos/cv169_23778.txt', 'pos/cv170_3006.txt', 'pos/cv171_13537.txt', 'pos/cv172_11131.txt', 'pos/cv173_4471.txt', 'pos/cv174_9659.txt', 'pos/cv175_6964.txt', 'pos/cv176_12857.txt', 'pos/cv177_10367.txt', 'pos/cv178_12972.txt', 'pos/cv179_9228.txt', 'pos/cv180_16113.txt', 'pos/cv181_14401.txt', 'pos/cv182_7281.txt', 'pos/cv183_18612.txt', 'pos/cv184_2673.txt', 'pos/cv185_28654.txt', 'pos/cv186_2269.txt', 'pos/cv187_12829.txt', 'pos/cv188_19226.txt', 'pos/cv189_22934.txt', 'pos/cv190_27052.txt', 'pos/cv191_29719.txt', 'pos/cv192_14395.txt', 'pos/cv193_5416.txt', 'pos/cv194_12079.txt', 'pos/cv195_14528.txt', 'pos/cv196_29027.txt', 'pos/cv197_29328.txt', 'pos/cv198_18180.txt', 'pos/cv199_9629.txt', 'pos/cv200_2915.txt', 'pos/cv201_6997.txt', 'pos/cv202_10654.txt', 'pos/cv203_17986.txt', 'pos/cv204_8451.txt', 'pos/cv205_9457.txt', 'pos/cv206_14293.txt', 'pos/cv207_29284.txt', 'pos/cv208_9020.txt', 'pos/cv209_29118.txt', 'pos/cv210_9312.txt', 'pos/cv211_9953.txt', 'pos/cv212_10027.txt', 'pos/cv213_18934.txt', 'pos/cv214_12294.txt', 'pos/cv215_22240.txt', 'pos/cv216_18738.txt', 'pos/cv217_28842.txt', 'pos/cv218_24352.txt', 'pos/cv219_18626.txt', 'pos/cv220_29059.txt', 'pos/cv221_2695.txt', 'pos/cv222_17395.txt', 'pos/cv223_29066.txt', 'pos/cv224_17661.txt', 'pos/cv225_29224.txt', 'pos/cv226_2618.txt', 'pos/cv227_24215.txt', 'pos/cv228_5806.txt', 'pos/cv229_13611.txt', 'pos/cv230_7428.txt', 'pos/cv231_10425.txt', 'pos/cv232_14991.txt', 'pos/cv233_15964.txt', 'pos/cv234_20643.txt', 'pos/cv235_10217.txt', 'pos/cv236_11565.txt', 'pos/cv237_19221.txt', 'pos/cv238_12931.txt', 'pos/cv239_3385.txt', 'pos/cv240_14336.txt', 'pos/cv241_23130.txt', 'pos/cv242_10638.txt', 'pos/cv243_20728.txt', 'pos/cv244_21649.txt', 'pos/cv245_8569.txt', 'pos/cv246_28807.txt', 'pos/cv247_13142.txt', 'pos/cv248_13987.txt', 'pos/cv249_11640.txt', 'pos/cv250_25616.txt', 'pos/cv251_22636.txt', 'pos/cv252_23779.txt', 'pos/cv253_10077.txt', 'pos/cv254_6027.txt', 'pos/cv255_13683.txt', 'pos/cv256_14740.txt', 'pos/cv257_10975.txt', 'pos/cv258_5792.txt', 'pos/cv259_10934.txt', 'pos/cv260_13959.txt', 'pos/cv261_10954.txt', 'pos/cv262_12649.txt', 'pos/cv263_19259.txt', 'pos/cv264_12801.txt', 'pos/cv265_10814.txt', 'pos/cv266_25779.txt', 'pos/cv267_14952.txt', 'pos/cv268_18834.txt', 'pos/cv269_21732.txt', 'pos/cv270_6079.txt', 'pos/cv271_13837.txt', 'pos/cv272_18974.txt', 'pos/cv273_29112.txt', 'pos/cv274_25253.txt', 'pos/cv275_28887.txt', 'pos/cv276_15684.txt', 'pos/cv277_19091.txt', 'pos/cv278_13041.txt', 'pos/cv279_18329.txt', 'pos/cv280_8267.txt', 'pos/cv281_23253.txt', 'pos/cv282_6653.txt', 'pos/cv283_11055.txt', 'pos/cv284_19119.txt', 'pos/cv285_16494.txt', 'pos/cv286_25050.txt', 'pos/cv287_15900.txt', 'pos/cv288_18791.txt', 'pos/cv289_6463.txt', 'pos/cv290_11084.txt', 'pos/cv291_26635.txt', 'pos/cv292_7282.txt', 'pos/cv293_29856.txt', 'pos/cv294_11684.txt', 'pos/cv295_15570.txt', 'pos/cv296_12251.txt', 'pos/cv297_10047.txt', 'pos/cv298_23111.txt', 'pos/cv299_16214.txt', 'pos/cv300_22284.txt', 'pos/cv301_12146.txt', 'pos/cv302_25649.txt', 'pos/cv303_27520.txt', 'pos/cv304_28706.txt', 'pos/cv305_9946.txt', 'pos/cv306_10364.txt', 'pos/cv307_25270.txt', 'pos/cv308_5016.txt', 'pos/cv309_22571.txt', 'pos/cv310_13091.txt', 'pos/cv311_16002.txt', 'pos/cv312_29377.txt', 'pos/cv313_18198.txt', 'pos/cv314_14422.txt', 'pos/cv315_11629.txt', 'pos/cv316_6370.txt', 'pos/cv317_24049.txt', 'pos/cv318_10493.txt', 'pos/cv319_14727.txt', 'pos/cv320_9530.txt', 'pos/cv321_12843.txt', 'pos/cv322_20318.txt', 'pos/cv323_29805.txt', 'pos/cv324_7082.txt', 'pos/cv325_16629.txt', 'pos/cv326_13295.txt', 'pos/cv327_20292.txt', 'pos/cv328_10373.txt', 'pos/cv329_29370.txt', 'pos/cv330_29809.txt', 'pos/cv331_8273.txt', 'pos/cv332_16307.txt', 'pos/cv333_8916.txt', 'pos/cv334_10001.txt', 'pos/cv335_14665.txt', 'pos/cv336_10143.txt', 'pos/cv337_29181.txt', 'pos/cv338_8821.txt', 'pos/cv339_21119.txt', 'pos/cv340_13287.txt', 'pos/cv341_24430.txt', 'pos/cv342_19456.txt', 'pos/cv343_10368.txt', 'pos/cv344_5312.txt', 'pos/cv345_9954.txt', 'pos/cv346_18168.txt', 'pos/cv347_13194.txt', 'pos/cv348_18176.txt', 'pos/cv349_13507.txt', 'pos/cv350_20670.txt', 'pos/cv351_15458.txt', 'pos/cv352_5524.txt', 'pos/cv353_18159.txt', 'pos/cv354_8132.txt', 'pos/cv355_16413.txt', 'pos/cv356_25163.txt', 'pos/cv357_13156.txt', 'pos/cv358_10691.txt', 'pos/cv359_6647.txt', 'pos/cv360_8398.txt', 'pos/cv361_28944.txt', 'pos/cv362_15341.txt', 'pos/cv363_29332.txt', 'pos/cv364_12901.txt', 'pos/cv365_11576.txt', 'pos/cv366_10221.txt', 'pos/cv367_22792.txt', 'pos/cv368_10466.txt', 'pos/cv369_12886.txt', 'pos/cv370_5221.txt', 'pos/cv371_7630.txt', 'pos/cv372_6552.txt', 'pos/cv373_20404.txt', 'pos/cv374_25436.txt', 'pos/cv375_9929.txt', 'pos/cv376_19435.txt', 'pos/cv377_7946.txt', 'pos/cv378_20629.txt', 'pos/cv379_21963.txt', 'pos/cv380_7574.txt', 'pos/cv381_20172.txt', 'pos/cv382_7897.txt', 'pos/cv383_13116.txt', 'pos/cv384_17140.txt', 'pos/cv385_29741.txt', 'pos/cv386_10080.txt', 'pos/cv387_11507.txt', 'pos/cv388_12009.txt', 'pos/cv389_9369.txt', 'pos/cv390_11345.txt', 'pos/cv391_10802.txt', 'pos/cv392_11458.txt', 'pos/cv393_29327.txt', 'pos/cv394_5137.txt', 'pos/cv395_10849.txt', 'pos/cv396_17989.txt', 'pos/cv397_29023.txt', 'pos/cv398_15537.txt', 'pos/cv399_2877.txt', 'pos/cv400_19220.txt', 'pos/cv401_12605.txt', 'pos/cv402_14425.txt', 'pos/cv403_6621.txt', 'pos/cv404_20315.txt', 'pos/cv405_20399.txt', 'pos/cv406_21020.txt', 'pos/cv407_22637.txt', 'pos/cv408_5297.txt', 'pos/cv409_29786.txt', 'pos/cv410_24266.txt', 'pos/cv411_15007.txt', 'pos/cv412_24095.txt', 'pos/cv413_7398.txt', 'pos/cv414_10518.txt', 'pos/cv415_22517.txt', 'pos/cv416_11136.txt', 'pos/cv417_13115.txt', 'pos/cv418_14774.txt', 'pos/cv419_13394.txt', 'pos/cv420_28795.txt', 'pos/cv421_9709.txt', 'pos/cv422_9381.txt', 'pos/cv423_11155.txt', 'pos/cv424_8831.txt', 'pos/cv425_8250.txt', 'pos/cv426_10421.txt', 'pos/cv427_10825.txt', 'pos/cv428_11347.txt', 'pos/cv429_7439.txt', 'pos/cv430_17351.txt', 'pos/cv431_7085.txt', 'pos/cv432_14224.txt', 'pos/cv433_10144.txt', 'pos/cv434_5793.txt', 'pos/cv435_23110.txt', 'pos/cv436_19179.txt', 'pos/cv437_22849.txt', 'pos/cv438_8043.txt', 'pos/cv439_15970.txt', 'pos/cv440_15243.txt', 'pos/cv441_13711.txt', 'pos/cv442_13846.txt', 'pos/cv443_21118.txt', 'pos/cv444_9974.txt', 'pos/cv445_25882.txt', 'pos/cv446_11353.txt', 'pos/cv447_27332.txt', 'pos/cv448_14695.txt', 'pos/cv449_8785.txt', 'pos/cv450_7890.txt', 'pos/cv451_10690.txt', 'pos/cv452_5088.txt', 'pos/cv453_10379.txt', 'pos/cv454_2053.txt', 'pos/cv455_29000.txt', 'pos/cv456_18985.txt', 'pos/cv457_18453.txt', 'pos/cv458_8604.txt', 'pos/cv459_20319.txt', 'pos/cv460_10842.txt', 'pos/cv461_19600.txt', 'pos/cv462_19350.txt', 'pos/cv463_10343.txt', 'pos/cv464_15650.txt', 'pos/cv465_22431.txt', 'pos/cv466_18722.txt', 'pos/cv467_25773.txt', 'pos/cv468_15228.txt', 'pos/cv469_20630.txt', 'pos/cv470_15952.txt', 'pos/cv471_16858.txt', 'pos/cv472_29280.txt', 'pos/cv473_7367.txt', 'pos/cv474_10209.txt', 'pos/cv475_21692.txt', 'pos/cv476_16856.txt', 'pos/cv477_22479.txt', 'pos/cv478_14309.txt', 'pos/cv479_5649.txt', 'pos/cv480_19817.txt', 'pos/cv481_7436.txt', 'pos/cv482_10580.txt', 'pos/cv483_16378.txt', 'pos/cv484_25054.txt', 'pos/cv485_26649.txt', 'pos/cv486_9799.txt', 'pos/cv487_10446.txt', 'pos/cv488_19856.txt', 'pos/cv489_17906.txt', 'pos/cv490_17872.txt', 'pos/cv491_12145.txt', 'pos/cv492_18271.txt', 'pos/cv493_12839.txt', 'pos/cv494_17389.txt', 'pos/cv495_14518.txt', 'pos/cv496_10530.txt', 'pos/cv497_26980.txt', 'pos/cv498_8832.txt', 'pos/cv499_10658.txt', 'pos/cv500_10251.txt', 'pos/cv501_11657.txt', 'pos/cv502_10406.txt', 'pos/cv503_10558.txt', 'pos/cv504_29243.txt', 'pos/cv505_12090.txt', 'pos/cv506_15956.txt', 'pos/cv507_9220.txt', 'pos/cv508_16006.txt', 'pos/cv509_15888.txt', 'pos/cv510_23360.txt', 'pos/cv511_10132.txt', 'pos/cv512_15965.txt', 'pos/cv513_6923.txt', 'pos/cv514_11187.txt', 'pos/cv515_17069.txt', 'pos/cv516_11172.txt', 'pos/cv517_19219.txt', 'pos/cv518_13331.txt', 'pos/cv519_14661.txt', 'pos/cv520_12295.txt', 'pos/cv521_15828.txt', 'pos/cv522_5583.txt', 'pos/cv523_16615.txt', 'pos/cv524_23627.txt', 'pos/cv525_16122.txt', 'pos/cv526_12083.txt', 'pos/cv527_10123.txt', 'pos/cv528_10822.txt', 'pos/cv529_10420.txt', 'pos/cv530_16212.txt', 'pos/cv531_26486.txt', 'pos/cv532_6522.txt', 'pos/cv533_9821.txt', 'pos/cv534_14083.txt', 'pos/cv535_19728.txt', 'pos/cv536_27134.txt', 'pos/cv537_12370.txt', 'pos/cv538_28667.txt', 'pos/cv539_20347.txt', 'pos/cv540_3421.txt', 'pos/cv541_28835.txt', 'pos/cv542_18980.txt', 'pos/cv543_5045.txt', 'pos/cv544_5108.txt', 'pos/cv545_12014.txt', 'pos/cv546_11767.txt', 'pos/cv547_16324.txt', 'pos/cv548_17731.txt', 'pos/cv549_21443.txt', 'pos/cv550_22211.txt', 'pos/cv551_10565.txt', 'pos/cv552_10016.txt', 'pos/cv553_26915.txt', 'pos/cv554_13151.txt', 'pos/cv555_23922.txt', 'pos/cv556_14808.txt', 'pos/cv557_11449.txt', 'pos/cv558_29507.txt', 'pos/cv559_0050.txt', 'pos/cv560_17175.txt', 'pos/cv561_9201.txt', 'pos/cv562_10359.txt', 'pos/cv563_17257.txt', 'pos/cv564_11110.txt', 'pos/cv565_29572.txt', 'pos/cv566_8581.txt', 'pos/cv567_29611.txt', 'pos/cv568_15638.txt', 'pos/cv569_26381.txt', 'pos/cv570_29082.txt', 'pos/cv571_29366.txt', 'pos/cv572_18657.txt', 'pos/cv573_29525.txt', 'pos/cv574_22156.txt', 'pos/cv575_21150.txt', 'pos/cv576_14094.txt', 'pos/cv577_28549.txt', 'pos/cv578_15094.txt', 'pos/cv579_11605.txt', 'pos/cv580_14064.txt', 'pos/cv581_19381.txt', 'pos/cv582_6559.txt', 'pos/cv583_29692.txt', 'pos/cv584_29722.txt', 'pos/cv585_22496.txt', 'pos/cv586_7543.txt', 'pos/cv587_19162.txt', 'pos/cv588_13008.txt', 'pos/cv589_12064.txt', 'pos/cv590_19290.txt', 'pos/cv591_23640.txt', 'pos/cv592_22315.txt', 'pos/cv593_10987.txt', 'pos/cv594_11039.txt', 'pos/cv595_25335.txt', 'pos/cv596_28311.txt', 'pos/cv597_26360.txt', 'pos/cv598_16452.txt', 'pos/cv599_20988.txt', 'pos/cv600_23878.txt', 'pos/cv601_23453.txt', 'pos/cv602_8300.txt', 'pos/cv603_17694.txt', 'pos/cv604_2230.txt', 'pos/cv605_11800.txt', 'pos/cv606_15985.txt', 'pos/cv607_7717.txt', 'pos/cv608_23231.txt', 'pos/cv609_23877.txt', 'pos/cv610_2287.txt', 'pos/cv611_21120.txt', 'pos/cv612_5461.txt', 'pos/cv613_21796.txt', 'pos/cv614_10626.txt', 'pos/cv615_14182.txt', 'pos/cv616_29319.txt', 'pos/cv617_9322.txt', 'pos/cv618_8974.txt', 'pos/cv619_12462.txt', 'pos/cv620_24265.txt', 'pos/cv621_14368.txt', 'pos/cv622_8147.txt', 'pos/cv623_15356.txt', 'pos/cv624_10744.txt', 'pos/cv625_12440.txt', 'pos/cv626_7410.txt', 'pos/cv627_11620.txt', 'pos/cv628_19325.txt', 'pos/cv629_14909.txt', 'pos/cv630_10057.txt', 'pos/cv631_4967.txt', 'pos/cv632_9610.txt', 'pos/cv633_29837.txt', 'pos/cv634_11101.txt', 'pos/cv635_10022.txt', 'pos/cv636_15279.txt', 'pos/cv637_1250.txt', 'pos/cv638_2953.txt', 'pos/cv639_10308.txt', 'pos/cv640_5378.txt', 'pos/cv641_12349.txt', 'pos/cv642_29867.txt', 'pos/cv643_29349.txt', 'pos/cv644_17154.txt', 'pos/cv645_15668.txt', 'pos/cv646_15065.txt', 'pos/cv647_13691.txt', 'pos/cv648_15792.txt', 'pos/cv649_12735.txt', 'pos/cv650_14340.txt', 'pos/cv651_10492.txt', 'pos/cv652_13972.txt', 'pos/cv653_19583.txt', 'pos/cv654_18246.txt', 'pos/cv655_11154.txt', 'pos/cv656_24201.txt', 'pos/cv657_24513.txt', 'pos/cv658_10532.txt', 'pos/cv659_19944.txt', 'pos/cv660_21893.txt', 'pos/cv661_2450.txt', 'pos/cv662_13320.txt', 'pos/cv663_13019.txt', 'pos/cv664_4389.txt', 'pos/cv665_29538.txt', 'pos/cv666_18963.txt', 'pos/cv667_18467.txt', 'pos/cv668_17604.txt', 'pos/cv669_22995.txt', 'pos/cv670_25826.txt', 'pos/cv671_5054.txt', 'pos/cv672_28083.txt', 'pos/cv673_24714.txt', 'pos/cv674_10732.txt', 'pos/cv675_21588.txt', 'pos/cv676_21090.txt', 'pos/cv677_17715.txt', 'pos/cv678_13419.txt', 'pos/cv679_28559.txt', 'pos/cv680_10160.txt', 'pos/cv681_9692.txt', 'pos/cv682_16139.txt', 'pos/cv683_12167.txt', 'pos/cv684_11798.txt', 'pos/cv685_5947.txt', 'pos/cv686_13900.txt', 'pos/cv687_21100.txt', 'pos/cv688_7368.txt', 'pos/cv689_12587.txt', 'pos/cv690_5619.txt', 'pos/cv691_5043.txt', 'pos/cv692_15451.txt', 'pos/cv693_18063.txt', 'pos/cv694_4876.txt', 'pos/cv695_21108.txt', 'pos/cv696_29740.txt', 'pos/cv697_11162.txt', 'pos/cv698_15253.txt', 'pos/cv699_7223.txt', 'pos/cv700_21947.txt', 'pos/cv701_14252.txt', 'pos/cv702_11500.txt', 'pos/cv703_16143.txt', 'pos/cv704_15969.txt', 'pos/cv705_11059.txt', 'pos/cv706_24716.txt', 'pos/cv707_10678.txt', 'pos/cv708_28729.txt', 'pos/cv709_10529.txt', 'pos/cv710_22577.txt', 'pos/cv711_11665.txt', 'pos/cv712_22920.txt', 'pos/cv713_29155.txt', 'pos/cv714_18502.txt', 'pos/cv715_18179.txt', 'pos/cv716_10514.txt', 'pos/cv717_15953.txt', 'pos/cv718_11434.txt', 'pos/cv719_5713.txt', 'pos/cv720_5389.txt', 'pos/cv721_29121.txt', 'pos/cv722_7110.txt', 'pos/cv723_8648.txt', 'pos/cv724_13681.txt', 'pos/cv725_10103.txt', 'pos/cv726_4719.txt', 'pos/cv727_4978.txt', 'pos/cv728_16133.txt', 'pos/cv729_10154.txt', 'pos/cv730_10279.txt', 'pos/cv731_4136.txt', 'pos/cv732_12245.txt', 'pos/cv733_9839.txt', 'pos/cv734_21568.txt', 'pos/cv735_18801.txt', 'pos/cv736_23670.txt', 'pos/cv737_28907.txt', 'pos/cv738_10116.txt', 'pos/cv739_11209.txt', 'pos/cv740_12445.txt', 'pos/cv741_11890.txt', 'pos/cv742_7751.txt', 'pos/cv743_15449.txt', 'pos/cv744_10038.txt', 'pos/cv745_12773.txt', 'pos/cv746_10147.txt', 'pos/cv747_16556.txt', 'pos/cv748_12786.txt', 'pos/cv749_17765.txt', 'pos/cv750_10180.txt', 'pos/cv751_15719.txt', 'pos/cv752_24155.txt', 'pos/cv753_10875.txt', 'pos/cv754_7216.txt', 'pos/cv755_23616.txt', 'pos/cv756_22540.txt', 'pos/cv757_10189.txt', 'pos/cv758_9671.txt', 'pos/cv759_13522.txt', 'pos/cv760_8597.txt', 'pos/cv761_12620.txt', 'pos/cv762_13927.txt', 'pos/cv763_14729.txt', 'pos/cv764_11739.txt', 'pos/cv765_19037.txt', 'pos/cv766_7540.txt', 'pos/cv767_14062.txt', 'pos/cv768_11751.txt', 'pos/cv769_8123.txt', 'pos/cv770_10451.txt', 'pos/cv771_28665.txt', 'pos/cv772_12119.txt', 'pos/cv773_18817.txt', 'pos/cv774_13845.txt', 'pos/cv775_16237.txt', 'pos/cv776_20529.txt', 'pos/cv777_10094.txt', 'pos/cv778_17330.txt', 'pos/cv779_17881.txt', 'pos/cv780_7984.txt', 'pos/cv781_5262.txt', 'pos/cv782_19526.txt', 'pos/cv783_13227.txt', 'pos/cv784_14394.txt', 'pos/cv785_22600.txt', 'pos/cv786_22497.txt', 'pos/cv787_13743.txt', 'pos/cv788_25272.txt', 'pos/cv789_12136.txt', 'pos/cv790_14600.txt', 'pos/cv791_16302.txt', 'pos/cv792_3832.txt', 'pos/cv793_13650.txt', 'pos/cv794_15868.txt', 'pos/cv795_10122.txt', 'pos/cv796_15782.txt', 'pos/cv797_6957.txt', 'pos/cv798_23531.txt', 'pos/cv799_18543.txt', 'pos/cv800_12368.txt', 'pos/cv801_25228.txt', 'pos/cv802_28664.txt', 'pos/cv803_8207.txt', 'pos/cv804_10862.txt', 'pos/cv805_19601.txt', 'pos/cv806_8842.txt', 'pos/cv807_21740.txt', 'pos/cv808_12635.txt', 'pos/cv809_5009.txt', 'pos/cv810_12458.txt', 'pos/cv811_21386.txt', 'pos/cv812_17924.txt', 'pos/cv813_6534.txt', 'pos/cv814_18975.txt', 'pos/cv815_22456.txt', 'pos/cv816_13655.txt', 'pos/cv817_4041.txt', 'pos/cv818_10211.txt', 'pos/cv819_9364.txt', 'pos/cv820_22892.txt', 'pos/cv821_29364.txt', 'pos/cv822_20049.txt', 'pos/cv823_15569.txt', 'pos/cv824_8838.txt', 'pos/cv825_5063.txt', 'pos/cv826_11834.txt', 'pos/cv827_18331.txt', 'pos/cv828_19831.txt', 'pos/cv829_20289.txt', 'pos/cv830_6014.txt', 'pos/cv831_14689.txt', 'pos/cv832_23275.txt', 'pos/cv833_11053.txt', 'pos/cv834_22195.txt', 'pos/cv835_19159.txt', 'pos/cv836_12968.txt', 'pos/cv837_27325.txt', 'pos/cv838_24728.txt', 'pos/cv839_21467.txt', 'pos/cv840_16321.txt', 'pos/cv841_3967.txt', 'pos/cv842_5866.txt', 'pos/cv843_15544.txt', 'pos/cv844_12690.txt', 'pos/cv845_14290.txt', 'pos/cv846_29497.txt', 'pos/cv847_1941.txt', 'pos/cv848_10036.txt', 'pos/cv849_15729.txt', 'pos/cv850_16466.txt', 'pos/cv851_20469.txt', 'pos/cv852_27523.txt', 'pos/cv853_29233.txt', 'pos/cv854_17740.txt', 'pos/cv855_20661.txt', 'pos/cv856_29013.txt', 'pos/cv857_15958.txt', 'pos/cv858_18819.txt', 'pos/cv859_14107.txt', 'pos/cv860_13853.txt', 'pos/cv861_1198.txt', 'pos/cv862_14324.txt', 'pos/cv863_7424.txt', 'pos/cv864_3416.txt', 'pos/cv865_2895.txt', 'pos/cv866_29691.txt', 'pos/cv867_16661.txt', 'pos/cv868_11948.txt', 'pos/cv869_23611.txt', 'pos/cv870_16348.txt', 'pos/cv871_24888.txt', 'pos/cv872_12591.txt', 'pos/cv873_18636.txt', 'pos/cv874_11236.txt', 'pos/cv875_5754.txt', 'pos/cv876_9390.txt', 'pos/cv877_29274.txt', 'pos/cv878_15694.txt', 'pos/cv879_14903.txt', 'pos/cv880_29800.txt', 'pos/cv881_13254.txt', 'pos/cv882_10026.txt', 'pos/cv883_27751.txt', 'pos/cv884_13632.txt', 'pos/cv885_12318.txt', 'pos/cv886_18177.txt', 'pos/cv887_5126.txt', 'pos/cv888_24435.txt', 'pos/cv889_21430.txt', 'pos/cv890_3977.txt', 'pos/cv891_6385.txt', 'pos/cv892_17576.txt', 'pos/cv893_26269.txt', 'pos/cv894_2068.txt', 'pos/cv895_21022.txt', 'pos/cv896_16071.txt', 'pos/cv897_10837.txt', 'pos/cv898_14187.txt', 'pos/cv899_16014.txt', 'pos/cv900_10331.txt', 'pos/cv901_11017.txt', 'pos/cv902_12256.txt', 'pos/cv903_17822.txt', 'pos/cv904_24353.txt', 'pos/cv905_29114.txt', 'pos/cv906_11491.txt', 'pos/cv907_3541.txt', 'pos/cv908_16009.txt', 'pos/cv909_9960.txt', 'pos/cv910_20488.txt', 'pos/cv911_20260.txt', 'pos/cv912_5674.txt', 'pos/cv913_29252.txt', 'pos/cv914_28742.txt', 'pos/cv915_8841.txt', 'pos/cv916_15467.txt', 'pos/cv917_29715.txt', 'pos/cv918_2693.txt', 'pos/cv919_16380.txt', 'pos/cv920_29622.txt', 'pos/cv921_12747.txt', 'pos/cv922_10073.txt', 'pos/cv923_11051.txt', 'pos/cv924_29540.txt', 'pos/cv925_8969.txt', 'pos/cv926_17059.txt', 'pos/cv927_10681.txt', 'pos/cv928_9168.txt', 'pos/cv929_16908.txt', 'pos/cv930_13475.txt', 'pos/cv931_17563.txt', 'pos/cv932_13401.txt', 'pos/cv933_23776.txt', 'pos/cv934_19027.txt', 'pos/cv935_23841.txt', 'pos/cv936_15954.txt', 'pos/cv937_9811.txt', 'pos/cv938_10220.txt', 'pos/cv939_10583.txt', 'pos/cv940_17705.txt', 'pos/cv941_10246.txt', 'pos/cv942_17082.txt', 'pos/cv943_22488.txt', 'pos/cv944_13521.txt', 'pos/cv945_12160.txt', 'pos/cv946_18658.txt', 'pos/cv947_10601.txt', 'pos/cv948_24606.txt', 'pos/cv949_20112.txt', 'pos/cv950_12350.txt', 'pos/cv951_10926.txt', 'pos/cv952_25240.txt', 'pos/cv953_6836.txt', 'pos/cv954_18628.txt', 'pos/cv955_25001.txt', 'pos/cv956_11609.txt', 'pos/cv957_8737.txt', 'pos/cv958_12162.txt', 'pos/cv959_14611.txt', 'pos/cv960_29007.txt', 'pos/cv961_5682.txt', 'pos/cv962_9803.txt', 'pos/cv963_6895.txt', 'pos/cv964_6021.txt', 'pos/cv965_26071.txt', 'pos/cv966_28832.txt', 'pos/cv967_5788.txt', 'pos/cv968_24218.txt', 'pos/cv969_13250.txt', 'pos/cv970_18450.txt', 'pos/cv971_10874.txt', 'pos/cv972_26417.txt', 'pos/cv973_10066.txt', 'pos/cv974_22941.txt', 'pos/cv975_10981.txt', 'pos/cv976_10267.txt', 'pos/cv977_4938.txt', 'pos/cv978_20929.txt', 'pos/cv979_18921.txt', 'pos/cv980_10953.txt', 'pos/cv981_14989.txt', 'pos/cv982_21103.txt', 'pos/cv983_22928.txt', 'pos/cv984_12767.txt', 'pos/cv985_6359.txt', 'pos/cv986_13527.txt', 'pos/cv987_6965.txt', 'pos/cv988_18740.txt', 'pos/cv989_15824.txt', 'pos/cv990_11591.txt', 'pos/cv991_18645.txt', 'pos/cv992_11962.txt', 'pos/cv993_29737.txt', 'pos/cv994_12270.txt', 'pos/cv995_21821.txt', 'pos/cv996_11592.txt', 'pos/cv997_5046.txt', 'pos/cv998_14111.txt', 'pos/cv999_13106.txt']\n"
     ]
    }
   ],
   "source": [
    "print(len(movie_reviews.fileids('pos')))\n",
    "print(' ')\n",
    "print(movie_reviews.fileids('pos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_rev = (movie_reviews.fileids('pos'))\n",
    "len(pos_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_rev = (movie_reviews.fileids('neg'))\n",
    "len(neg_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'computer', '-', 'animated', 'comedy', '\"', ...]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev = nltk.corpus.movie_reviews.words('pos/cv059_28885.txt')\n",
    "rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for rev in pos_rev:\n",
    "    rev_text_neg = rev = nltk.corpus.movie_reviews.words(rev)\n",
    "    rev_one_string = \" \".join(rev_text_neg)\n",
    "    rev_one_string = review_one_string.replace(' ,', ',')\n",
    "    rev_one_string = review_one_string.replace(' .', '.')\n",
    "    rev_one_string = review_one_string.replace(\"\\' \", \"'\")\n",
    "    rev_one_string = review_one_string.replace(\"\\' \", \"'\")\n",
    "    rev_list.append(rev_one_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_targets = np.zeros((1000,),dtype = np.int)\n",
    "pos_targets = np.ones((1000,),dtype = np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []\n",
    "for neg_tar in neg_targets:\n",
    "    target_list.append(neg_targets)\n",
    "for pos_tar in pos_targets:\n",
    "    target_list.append(pos_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.Series(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "\n",
    "Bag of words is a particular representation model used to simplify the contents of a selection of text. The bag of words model omits grammar and word order, but is interested in the number of occurrences of words within the text. The ultimate representation of the text selection is that of a bag of words (bag referring to the set theory concept of multisets, which differ from simple sets).\n",
    "\n",
    "Actual storage mechanisms for the bag of words representation can vary, but the following is a simple example using a dictionary for intuitiveness. Sample text:\n",
    "\n",
    "\"Well, well, well,\" said John.\n",
    "\"There, there,\" said James. \"There, there.\"\n",
    "\n",
    "The resulting bag of words representation as a dictionary:\n",
    "\n",
    "   {\n",
    "   \n",
    "      'well': 3,\n",
    "      \n",
    "      'said': 2,\n",
    "      \n",
    "      'john': 1,\n",
    "      \n",
    "      'there': 4,\n",
    "      \n",
    "      'james': 1\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bagofwords = ['''Beans. I was trying to explain to somebody as we were flying in, that’s corn. That’s beans.\n",
    "And they were very impressed at my agricultural knowledgeBeans. \n",
    "I was trying to explain to somebody as we were flying in, that’s corn. \n",
    "That’s beans. And they were very impressed at my agricultural knowledgeBeans. \n",
    "I was trying to explain to somebody as we were flying in, that’s corn. That’s beans. \n",
    "And they were very impressed at my agricultural knowledge''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model \n",
    "word2count = {} \n",
    "for data in Bagofwords: \n",
    "    words = nltk.word_tokenize(data) \n",
    "    for word in words: \n",
    "        if word not in word2count.keys(): \n",
    "            word2count[word] = 1\n",
    "        else: \n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Beans': 1,\n",
       " '.': 9,\n",
       " 'I': 3,\n",
       " 'was': 3,\n",
       " 'trying': 3,\n",
       " 'to': 6,\n",
       " 'explain': 3,\n",
       " 'somebody': 3,\n",
       " 'as': 3,\n",
       " 'we': 3,\n",
       " 'were': 6,\n",
       " 'flying': 3,\n",
       " 'in': 3,\n",
       " ',': 3,\n",
       " 'that': 3,\n",
       " '’': 6,\n",
       " 's': 6,\n",
       " 'corn': 3,\n",
       " 'That': 3,\n",
       " 'beans': 3,\n",
       " 'And': 3,\n",
       " 'they': 3,\n",
       " 'very': 3,\n",
       " 'impressed': 3,\n",
       " 'at': 3,\n",
       " 'my': 3,\n",
       " 'agricultural': 3,\n",
       " 'knowledgeBeans': 2,\n",
       " 'knowledge': 1}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "Before further processing, text needs to be normalized. Normalization generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctuation, expanding contractions, converting numbers to their word equivalents, and so on. Normalization puts all words on equal footing, and allows processing to proceed uniformly.b\n",
    "\n",
    "Convert text to lower case.\n",
    "\n",
    "Remove all non-word characters.\n",
    "\n",
    "Remove all punctuations. ? ! -  . , ' \"\n",
    "\n",
    "filter_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re \n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"  BEANS.# I was explain @ to somebody \" HAI\" as we were ##.+ very impressed at  -- my a!!! \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nltk.sent_tokenize(text) \n",
    "for i in range(len(dataset)): \n",
    "    dataset[i] = dataset[i].lower() \n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) \n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' beans i was explain to somebody hai as we were very impressed at my a ',\n",
       " ' ']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Expressions\n",
    "\n",
    "Regular expressions, often abbreviated regexp or regexp, are a tried and true method of concisely describing patterns of text. A regular expression is represented as a special text string itself, and is meant for developing search patterns on selections of text. Regular expressions can be thought of as an expanded set of rules beyond the wildcard characters of ? and *. Though often cited as frustrating to learn, regular expressions are incredibly powerful text searching tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('treebank')\n",
    "#!pip install patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Measures\n",
    "\n",
    "There are numerous similarity measures which can be applied to NLP. What are we measuring the similarity of? Generally, strings.\n",
    "\n",
    "Levenshtein - the number of characters that must be deleted, inserted, or substituted in order to make a pair of strings equal\n",
    "Jaccard - the measure of overlap between 2 sets; in the case of NLP, generally, documents are sets of words\n",
    "Smith Waterman - similar to Levenshtein, but with costs assigned to substitution, insertion, and deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match!\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"Cookie\"\n",
    "sequence = \"Cookie\"\n",
    "if re.match(pattern, sequence):\n",
    "    print(\"Match!\")\n",
    "else: print(\"Not a match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity:  0.2886751345948129\n"
     ]
    }
   ],
   "source": [
    "# Program to measure the similarity between  \n",
    "# two sentences using cosine similarity. \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "# X = input(\"Enter first string: \").lower() \n",
    "# Y = input(\"Enter second string: \").lower() \n",
    "X =\"I love horror movies\"\n",
    "Y =\"Lights out is a horror movie\"\n",
    "  \n",
    "# tokenization \n",
    "X_list = word_tokenize(X)  \n",
    "Y_list = word_tokenize(Y) \n",
    "  \n",
    "# sw contains the list of stopwords \n",
    "sw = stopwords.words('english')  \n",
    "l1 =[];l2 =[] \n",
    "  \n",
    "# remove stop words from the string \n",
    "X_set = {w for w in X_list if not w in sw}  \n",
    "Y_set = {w for w in Y_list if not w in sw} \n",
    "  \n",
    "# form a set containing keywords of both strings  \n",
    "rvector = X_set.union(Y_set)  \n",
    "for w in rvector: \n",
    "    if w in X_set: l1.append(1) # create a vector \n",
    "    else: l1.append(0) \n",
    "    if w in Y_set: l2.append(1) \n",
    "    else: l2.append(0) \n",
    "c = 0\n",
    "  \n",
    "# cosine formula  \n",
    "for i in range(len(rvector)): \n",
    "        c+= l1[i]*l2[i] \n",
    "cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "print(\"similarity: \", cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYNTACTIC & SEMANTIC ANALYSIS\n",
    "Syntactic analysis (syntax) and semantic analysis (semantic) are the two primary techniques that lead to the understanding of natural language. Language is a set of valid sentences, but what makes a sentence valid? Syntax and semantics.\n",
    "\n",
    "Syntax is the grammatical structure of the text, whereas semantics is the meaning being conveyed. A sentence that is syntactically correct, however, is not always semantically correct. For example, “cows flow supremely” is grammatically valid (subject — verb — adverb) but it doesn't make any sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic Analysis\n",
    "\n",
    "Also referred to as parsing, syntactic analysis is the task of analyzing strings as symbols, and ensuring their conformance to a established set of grammatical rules. This step must, out of necessity, come before any further analysis which attempts to ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Analysis\n",
    "\n",
    "Also known as meaning generation, semantic analysis is interested in determining the meaning of text selections (either character or word sequences). After an input selection of text is read and parsed (analyzed syntactically), the text selection can then be interpreted for meaning. Simply put, syntactic analysis is concerned with what words a text selection was made up of, while semantic analysis wants to know what the collection of words actually means. The topic of semantic analysis is both broad and deep, with a wide variety of tools and techniques at the researcher's disposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is the process of evaluating and determining the sentiment captured in a selection of text, with sentiment defined as feeling or emotion. This sentiment can be simply positive (happy), negative (sad or angry), or neutral, or can be some more precise measurement along a scale, with neutral in the middle, and positive and negative increasing in either direction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" Each for his own remebering has a list of Lovely things and It's may be UNLIKE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Each',\n",
       " 'for',\n",
       " 'his',\n",
       " 'own',\n",
       " 'remebering',\n",
       " 'has',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'Lovely',\n",
       " 'things',\n",
       " 'and',\n",
       " \"It's\",\n",
       " 'may',\n",
       " 'be',\n",
       " 'UNLIKE',\n",
       " '']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex\n",
    "regex.split(\"[\\s\\.\\,]\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Each',\n",
       " 'for',\n",
       " 'his',\n",
       " 'own',\n",
       " 'remebering',\n",
       " 'has',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'Lovely',\n",
       " 'things',\n",
       " 'and',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'may',\n",
       " 'be',\n",
       " 'UNLIKE']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
